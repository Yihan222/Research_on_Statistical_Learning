---
title: "boosting"
author: "Yihan Zhu"
date: "12/2/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

```{r}
library(caret)
library(ggplot2)
library(lattice)
data <- read.csv("heart.csv",fileEncoding="UTF-8-BOM")
data$X <- NULL
data$X.1 <- NULL
data$X.2 <- NULL

set.seed(726716)
trainIndex = createDataPartition(data$target, p=0.7)$Resample1
train = data[trainIndex, ]
test = data[-trainIndex, ]
```

```{r}
## Implementing gradient boosting model

library(gbm)
require(gbm)
h_d.boost=gbm(target ~ . ,train,
                 distribution = "gaussian",n.trees = 10000,
                 shrinkage = 0.01, interaction.depth = 4)
## Showing variable importance / Relative Influence
summary(h_d.boost)

#Plot of Response variable with chol variable
plot(h_d.boost,i="thalach") 

#Plot of Response variable with thal variable
plot(h_d.boost,i="oldpeak")

n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 
```

```{r}
#Generating a Prediction matrix for each Tree
predmatrix<-predict(h_d.boost,train,n.trees = n.trees)
dim(predmatrix) #dimentions of the Prediction Matrix

#Calculating The Mean squared Test Error
test.error<-with(train,apply( (predmatrix-target)^2,2,mean))
head(test.error) #contains the Mean squared test error for each of the 100 trees averaged

#Plotting the test error vs number of trees
plot(n.trees , test.error , pch=19,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")

```

